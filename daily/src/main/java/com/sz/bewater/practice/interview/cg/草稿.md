---------------------------------jvm--------------------------------------------------------------------
加载-》链接(校验\准备\解析)->初始化
加载：JVM将所需的类加载到内存中。
链接：
验证：验证class文件的合法性。
准备：为类中的静态变量分配内存并赋予初始值（默认值）。
解析：将类的常量池中的符号引用替换成直接引用。
初始化：对静态变量赋予用户定义的初始值并执行静态代码块。

栈帧：局部变量表 方法出口  操作数栈（用于计算的一块区域 计算好了重新压入栈帧中的局部变量表中或者返回） 动态链接（用于方法调用时 将符号引用转为直接引用 也即找对应调用方法的实际内存地址）

方法区存储信息：
1.类元信息
2.方法信息
3.运行时常量池  (每个类都有个常量池 记录着一些字面量和符号引用  运行时常量池:当类被加载时  类的常量池中的信息就会被存储在方法区中的运行时常量池 同时符号引用会转为直接引用(如指向方法、字段的具体内存地址)
4.类的静态变量
5.JIT编译的代码（可以理解成优化后的字节码）

GCRoot
1.虚拟机栈引用的对象
2.本地方法栈引用的对象
3.类的常量引用的对象
4.类的静态变量引用的对象

复制算法：复制的是存活对象 新生代存活对象较少 都是转瞬即逝 所以用复制算法效率较高（复制较少存活的对象从from区到to区 然后清空from 然后from和同互换）

cms：初始标记（标记那些gcroot对象直接引用的对象 很快 虽然stw 但是很快）   并发标记（标记整个堆中存活的对象 由gcroot出发间接引用的对象 与用户线程同时运行）   
重新标记（修正并发标记期间因用户线程继续运行而导致标记出现变动的对象 stw）  并发清除（并发标记清除 与用户线程同时运行 会产生内存碎片 久而久之必然会fullgc解决内存碎片化问题）
注意标记的是存活对象 而不是垃圾对象
并发：gc线程和用户线程同时运行
并行：多组gc线程同时运行

G1垃圾回收器回收过程
1.新生代垃圾回收
刚开始 G1会选择几块空闲的region作为eden区，当eden区满了 则会触发minorGC 将存活的对象复制到一块新的region作为Survivor区，如果Survivor区也满了，则可能会将部分存活对象晋升到老年代。
在多次Minor GC后，如果对象仍然存活且满足一定的条件（如经过的GC次数），这些对象会被晋升到老年代。
2.新生代垃圾回收+并发标记
当老年代的内存使用超过阈值（默认45%）时，会触发并发标记。这个阶段包括初始标记（会暂停用户线程，但时间很短），根区域扫描，并发标记（不会暂停用户线程），和重新标记（会短暂暂停用户线程）。
重新标记阶段会处理并发标记期间可能出现的错标或漏标问题。
G1会根据用户设定的停顿时间目标，选择需要回收的区域。这些区域通常是那些垃圾最多存活对象最少（收益较高）的区域。
3.混合垃圾回收
并发标记结束后，会进行混合GC，回收Eden区、Survivor区和部分老年代的Region。
Eden区和Survivor区中的存活对象会被复制到新的Survivor区或晋升到老年代。老年代中的存活对象也可能被复制到新的Region，这样可以整理老年代中的碎片。

	G1在回收老年代时也会有像cms中回收老年代一样的初始标记并发标记和重新标记的过程

	不同的垃圾回收器会根据其设计目标和回收策略，采用不同的堆内存结构和划分方式。传统的分代垃圾回收器如Parallel GC和CMS采用固定的年轻代和老年代划分，
	而像G1、ZGC和Shenandoah则采用更灵活的区域化或动态内存管理方式，以实现更高效的内存回收和更低的停顿时间
	也就是说堆内存的划分会随着不同的垃圾回收器的参与而进行不同的划分

大对象：需要申请连续内存空间的对象 究竟多大算大对象？ 可由参数配置 例如G1垃圾回收器 参数g1heapregionsize 这个是一个region的大小 超过这个region的一半的可以算成大对象
G1垃圾回收器通常会将超过region区域大小的一半的对象是为大对象 将其放在一个或者多个Humongous区域  Humongous区域在回收时不会放入老年代 回收过程和老年代类似

啥时候fullgc：老年代空间不足   元空间（永久代不足 永久代逻辑上和堆分开 物理上是放堆里面的） 大对象无法分配（可能 为什么说可能因为垃圾回收器 可能会尝试内存碎片整理不一定fullgc） 显式调用System.gc(危险)  oom通常会伴随一次fullgc    程序长时间空闲(算是一种优化)


垃圾回收有两大重要指标: 响应时间优先 吞吐量优先
响应时间优先 侧重于减少单次垃圾回收的停顿时间 哪怕这种侧重点会增加垃圾回收的次数 如cms g1
吞吐量优先侧重于减少总体的垃圾回收的停顿时间 这样可以在程序正常运行时能处理更多的批次任务 例如parallel gc

元空间相较于永久代所具有的优势:
永久代的大小固定 设置的不当容易造成fullgc  而元空间使用的是本地内存 假如没设置最大值 是支持动态拓展的
垃圾回收在永久代满时触发，这可能导致频繁的Full GC，影响应用性能  元空间的垃圾回收触发条件可以通过动态调整(元空间会动态拓展、-XX:MetaspaceGCThreshold该参数可以控制元空间触发垃圾回收的阈值)，更加灵活，减少了Full GC的频率。

	LinkedBlockingQueue  默认是Integer的最大值  默认无界
	ArrayBlockingQueue    有界
	SynchronousQueue  不存储元素 一个take对应一个put 没有就阻塞在那里
	PriorityBlockingQueue   无界的按优先级的(可以理解为按我们自定义的顺序消费的)阻塞队列  阻塞队列中存放的元素需实现Comparable接口来自定义消费顺序
	DelayQueue  无界的延迟阻塞队列  延迟时间最短的最新消费  队列里的元素 需实现Delayed接口 重写getDelay和compareTo方法

死锁发生的四种必要条件
①.互斥 同一时间 只有一个线程获取到资源  如何破坏 不讨论 没必要破坏互斥 没意义
②.不可剥夺  在未释放之前 不可被其他线程获取  如何破坏:  可被剥夺 例如:1.高优先级的线程申请资源时 可破坏低优先级的线程请求中断且释放资源   2.设置超时机制:一段时间获取不到则停止申请资源并释放自身资源
③.请求与保持  线程在等待时,不会释放自己已占有的资源 如何破坏:1.设置线程申请资源时一次性申请全部所需资源 否则不做操作
④.循环等待 多个线程互相等待对方释放资源  如何破坏:1.将所有资源设置一个全局顺序 所有线程按顺序申请资源 例如，线程只能在获得资源1后请求资源2，避免循环等待。


线程的状态: new-->就绪(runnable 调用了start)-->运行(获得了cpu时间片 执行线程体run)--->阻塞(blocked wait\sleep)--->销毁(执行完任务 等待销毁terminated)


数据库的三大范式
第一范式：要求数据库表中的每一列都是不可分割的原子值。也就是说，每个字段只能包含一个值，而不能是一个集合、数组或其他复杂数据结构。 确保每一列都是原子的，不可分割。
第二范式：第二范式在满足第一范式的基础上，要求表中的每一个非主键列都完全依赖于主键，而不能依赖于主键的一部分（对于复合主键而言）。 确保每个非主键列完全依赖于主键，而不是主键的一部分。
第三范式：第三范式在满足第二范式的基础上，要求表中的非主键列之间不存在传递依赖关系（即非主键列不能依赖于其他非主键列）。           确保非主键列之间没有传递依赖关系，每个非主键列只依赖于主键。

普通索引  唯一索引（允许有空值） 主键索引（不允许有空值的唯一索引） 联合索引（组合索引）


innodb的四种隔离级别：
1.读未提交（最低 造成脏读、不可重复读、幻读）
2.读已提交（造成 不可重复读、幻读）
3.可重复读（mysql默认的隔离级别 造成幻读）
4.串行化（最高 同一时间点只允许一个事务操作  不会产生数据不一致）
幻读和不可重复读的理解：  幻读是导致最终结果条数多了   不可重复读是导致最终结果值与预期不一致    mvcc（多版本并发控制解决不可重复读）  间隙锁解决幻读
mvcc ：给每个事务分配一个唯一的版本  每个事物在查询的时候根据自己的版本选择自己版本的数据 以事务对应的版本号来保证每次读取的数据都是我们预期的
间隙锁：会对范围内的数据加锁 也会对范围内不存在数据的间隙进行加锁 方式本次事务操作时 有其他事务来进行插入插入数据导致幻读

mvcc 多版本并发控制  
通过undo log实现:  mvcc指的是每个事务id对应一个数据版本号  也就是同一数据行 在事务并发情况下看到的数据是不一样(快照读 一致性读 也即是读到的数据不必是最新的)  而undo log除了存数据变更之前的数据 不同的版本号也存在undolog中
在数据库层面  每张数据表都有两个隐藏的字段 一个是最近一次操作的事务id 一个是undolog中相对应的版本号指针  从而达到事务并发控制的目的

聚簇索引：叶子节点存储索引值和整行数据  也即数据和索引存放在一起   一张表有且仅有一个聚簇索引（innodb 因为叶子节点包含了整行数据 而索引又是排好序的 所以数据行也是排好序的  数据行的物理顺序是固定的 故聚簇索引只能有一个）
非聚簇索引（辅助索引）：叶子节点存储的是索引值和主键值（找到相应的数据  需要根据主键值 再去聚簇索引树上再去查找一次 也即回表） 数据和索引不放在一起
聚簇索引和非聚簇索引都是B+树的结构  只是存储的内容不一样
一张表必须要有一个聚簇索引 一般是主键作为聚簇索引 如果没有设置主键 则选择唯一索引（不为null）作为聚簇索引  假设也没有唯一索引 则系统会隐式的创建一个聚簇索引
非聚簇索引可以有多个

最左匹配原则：
一般用于复合索引  查询条件在包含最左的索引字段即可 哪怕顺序不是第一个也行 或者未包含全部的列(有最左边的列即可) 因为有底层优化器的存在 包含了最左的索引字段 联合索引(组合索引)就会生效

索引下推：
一种优化手段  减少回表次数  假设有一个联合索引 根据第一个查询条件从索引树找到满足条件的主键值 假如没有索引下推 就会陆续回表了  
然而还有另一个查询条件存在  会再执行一次过滤 过滤出满足全部条件的主键值 从而减少了回表次数

覆盖索引：
是指一种查询优化  当我们要查询的结果就是我们的主键或者是我们的辅助索引 这样我们就不需要回表再去查一次了  避免回表
ps：我们知道  当模糊查询是%放在查询字段前面 则会导致索引失效（因为索引在B+树上是排好序的 这种%放在查询字段前面 没法定位叶子节点位置 则会导致索引失效）   而此时当要查询结果就是我们的查询字段 则出现索引覆盖 这时候就不会出现全部扫描了
explain select * from sales where product like '%A';  type为ALL   对product字段建了普通索引
explain select product from sales where product like '%A';   type为index  extra:using index  用到了覆盖索引
explain select id from sales where product like '%A';		type为index extra:using index  用到了覆盖索引   id为主键  这里也不需要回表了 覆盖索引
覆盖索引+子查询还可以解决超大分页问题
超大分页： 分页越往后 效率越低（需要排序的数量比较多  留下来的很少 limit 900000,10 排序1-900010条 只留下900001-9000010）
sql：select * from table t,（select id from table order by id limit 900000,10）a where a.id = t.id  在子查询中使用了覆盖索引 select id


B树和B+树都是多叉平衡树
B+树相较于B树 做了以下改进：
①.数据只存在与叶子节点 相较于B树(数据可以存在与非叶子节点) 意味着B+树相较于B树有更多的扇出  可以储存更多的数据 可以有效的降低树的高度 减少IO次数
②.叶子节点之间通过指针相连形成链表  为了范围查找提供了便利
B+树每个节点通常试着为一个数据页的大小(通常为16Kb) 树高为2的B+树通常可以储存几千万w条数据行(假设每条数据行大小为1k)  树高为3的B+树通常可以储存几千亿条数据
树高为3指的是从根节点到叶子节点一共有三层  每一层都是一次IO操作  以主键索引来说 通常需要3次IO操作就能找到想要的数据行   假如是范围查找 那就是3次+(符合范围条件的节点数目 从一个节点到另一个节点就算是一次IO操作 因为前面说了一个节点通常是一个数据页)
假如是辅助索引的话 那就是辅助索引上的IO次数再加上回表到主键索引树上的IO次数


explain sql执行计划
type有以下几种类型
system:查询系统表  效率最高 最多返回一条
const: 使用到了主键索引   最多一条
eq_ref: 使用到了唯一索引  最多一条
ref: 使用到了辅助索引  可能返回多条
range: 索引的范围查询  可能返回多条
index: 查找所有索引树
all: 全表扫描
效率(一般来说): system>const>eq_ref>ref>range>index>all   优化的时候至少优化到range级别  为什么说是一般来说  因为type为index不一定效率就不行  有可能是发生了索引覆盖 这时候效率还是可以的
除了type还有个比较重要的字段extra  它记录了sql的详细执行过程  常见的有using where(通常意味着全部扫描)  using index(覆盖索引)
possibleKeys：可能用到的索引集
key：实际用到的索引

sql调优:
慢查询开关打开  可设置慢查询sql最大容忍时间  慢查询日志可选file(记录到文件)、table（记录到表中）   可设置未使用索引的sql查询 记录到日志中
1.可以将经常需要排序的字段 设置成索引 因为索引本身是排好序的 速度会很快
2.尽量不要用select *  返回的列没有创建索引 导致回表 用不到覆盖索引
3。尽量避免复杂的join和子查询（join操作不要超过三张表）
4.合理的使用索引（联系索引的特性和结构回答）
5.如果用到数据的联合 且不需要过滤null 同union all代替union

sql调优延伸到join连接查询调优：
①.永远都是小结果集驱动大结果集  例如A left join B 那么A就是驱动表  B就是被驱动表  这时候A小于B是比较好的  小表驱动大表 做的数据库连接操作就会小一些
②.用于连接的字段设置索引
③.mysql有join buffer这种优化手段 但我们没有在join字段上设置索引 mysql不会简单粗暴的使用双重for循环的方式(可以理解成双重for循环)来找出结果集   而是引入了join buffer 每次都是批量的和被驱动表来筛选数据
我们可以增大这个join buffer  也就是相当于增加每批次比较数量  来提高效率

注意不仅仅是where语句后面跟索引 才会走索引  :
1.order by 索引: 也会走索引
2.join 连接条件上有索引 也会走索引
3.group by 索引
4.distinct 索引


哪些情况下会使得索引失效
①.当mysql优化器觉得全表扫描更快的时候 就不会走索引 例如select *
②.使用or连接条件 不会走索引
③.当使用联合索引 不满足最左原则的时候 不走索引
④.当对索引字段进行运算的时候 不走索引
⑤.模糊查找 %在查询字段左边(%字段、 %字段%)则不走索引(覆盖索引除外)   字段%这种可以
⑥.当使用!=（<>和!=都是表示不等于的意思） 、is null、is not null  不走索引


undo log、redo log、bin log
undo log：①.提供事务回滚操作   ②.提供MVCC(多版本并发控制)
redo log: ①.用于数据库灾后恢复(需配合undo log 既会恢复已经提交的事务 也会重做未提交的事务 然后配合undo log回滚未提交事务 来使数据库恢复到宕机时的状态)  ②.提高数据库写性能(数据库写操作会先写道redo log buffer 是一种顺序写 而数据写入到磁盘是随机写 很明显顺序写效率优于随机写)
bin log: 是一种二进制日志文件  记录每次数据库操作的sql语句  注意只有事务提交后才会写到bin log  作用:①.主从同步(将主库写操作通过bin log同步到从库)  ②.数据恢复(可将数据恢复到某一时间点)
在 MySQL 中，undo log 用于回滚未提交的事务。
当事务进行回滚操作时，MySQL 会根据 undo log 中记录的信息，将数据恢复到事务开始之前的状态。具体来说，undo log 中记录了事务对数据进行的修改操作，例如插入、更新和删除等。通过这些记录，MySQL 可以撤销这些修改，从而实现事务的回滚。
需要注意的是，undo log 只用于回滚未提交的事务。对于已经提交的事务，MySQL 不会使用 undo log 进行回滚，因为这些事务的修改已经被持久化到数据库中。

Redo log 记录的是更底层的数据页级别的修改细节，能够更精确地进行恢复操作；而 Binlog 记录的是相对较高层次的语句级别的操作。
比如说，在一个事务中对多行数据进行了多次更新，如果仅依靠 Binlog 来恢复，可能会出现一些不一致的情况，而 Redo log 可以准确地恢复到每个数据页的具体状态。
在进行恢复时，Redo log 可以精确地将数据页恢复到事务中断前的准确状态，确保数据的一致性和完整性。


redis的持久化方式
1.rdb（redis database backup）的redis数据以快照的方式记录在dump.rdb文件中  所以这种方式 在重启后恢复速度会很快  而且生成的文件是紧凑的二进制文件  占用空间较小  
但是受限于redis中的数据量 当数据量较大时 生成快照速度会稍慢
在生成快照时假如redis宕机 则会丢失最近一次修改的数据  安全性不太高
可以以fork子进程的方式来做持久化  不阻塞主进程的业务

2.aof（append only file）：将redis的每次修改操作记录在日志文件中  重启时重放该日志文件来达到恢复数据的目的  appendonly yes 开启aof
可通过细粒度的配置（如always、every sec、no（系统自己决定）） 来达到按需求 最小化的减少数据的丢失
因为是重放日志文件  所以当文件比较大时 恢复数据会比较慢
因为是以追加日志的方式 所以aof文件大小会随时间而越来越大(需定期重写rewrite)   因为aof会记录每次操作的命令 加入对某个key多次set 他也会记录多次 但是只有最后一次才有意义  aof重写就会去掉这些没有意义额度操作 从而减少aof的大小  重写阈值也可配置

redis的删除策略:
当redis中的数据 达到过期时间 redis并不是马上删除数据
1.惰性删除  当我们查询数据 redis发现数据过期了 才会去删除数据
2.定期删除  Redis 启动一个后台任务，每隔一段时间（默认 100 毫秒）随机抽取一些键进行检查，如果键已经过期则删除
这个操作不会检查所有键，而是随机抽取一部分，因此即使存在大量过期键，也不会导致一次性删除大量键而影响性能。
如果在某次扫描中删除了大量过期键，可能会触发更多的扫描以加速清理。
可以通过配置参数来调整定期删除的频率和检查的密度，以适应不同的使用场景和性能需求
redis是惰性删除+定期删除配合使用

redis中的淘汰策略
当内存不够用时(达到redis设置的最大内存时)  redis会淘汰一些数据
lru(least rencent used)   lfu(least frequently used)  ttl(time to live)
1.noeviction(默认)  也即不主动删除数据  而是相当于一种拒绝策略  当内存不够用时  数据写入会报错
2.allkeys-lru  范围为redis中所有的key 最近最少使用的数据 率先被淘汰
3.volatile-lru  范围为设置了过期时间的key  最近最少使用的数据 率先被淘汰
4.allkeys-random  范围为redis中所有key  随机被淘汰
5.volatile-random  范围为设置了过期时间的key  随机被淘汰
6.volatile-ttl  范围为设置了过期时间的key  最接近过期时间的key  率先被淘汰
7.allkeys-lfu	范围为所有key 最少使用频率的key 率先被淘汰
8.volatile-lfu  范围为设置了过期时间的key  最少使用频率的key 率先被淘汰
maxmemory：用于设置 Redis 可以使用的最大内存。
maxmemory-policy：用于设置具体的淘汰策略，如 noeviction、allkeys-lru 等。

---------------------------------------------------------------spring---------------------------------------------------------------------------------------------

spring是如何解决循环依赖的?
三级缓存
一级缓存:存放的是经历了完整的bean生命周期的bean
二级缓存:存放的是未经历完整周期 提前暴露出来的bean的引用地址  bean的早期引用
三级缓存:key是beanName  value为创建bean的工厂 objectFactory 或者说是一个lamda表达式 用来获取bean或者bean的代理对象 获取完放入二级缓存
(我们常说三级缓存里存的是一个lamda表达式 其实存放的是ObjectFactory接口 这个接口又是个函数式接口getObject 所以说三级缓存里放的是lamda表达式)
假设A依赖于B  B依赖于A
1.实例化 AService：
Spring 通过反射实例化 AService，此时 AService 对象已经创建，但还未完成属性赋值和初始化。
2.提前暴露 AService：
Spring 将 AService 的创建工厂（一个 Lambda 表达式）放入三级缓存（singletonFactories），用于在后续的依赖注入过程中获取 AService 的早期引用。
例如：singletonFactories.put("aService", () -> getEarlyBeanReference("aService", aService));
3.属性注入 AService：
在给 AService 注入属性时，Spring 发现 AService 依赖 BService。
4.创建 BService：
Spring 开始创建 BService，同样通过反射实例化 BService，并将其创建工厂放入三级缓存。
5.发现 BService 依赖 AService：
在给 BService 注入属性时，Spring 发现 BService 依赖 AService。
Spring 会首先从一级缓存中查找 AService，但此时 AService 尚未完全初始化，不在一级缓存中。
然后 Spring 从二级缓存中查找 AService，也找不到。
最后，Spring 从三级缓存中查找 AService 的 ObjectFactory，找到并调用其 getObject 方法。
6.获取 AService 的早期引用：
调用 ObjectFactory.getObject 方法获取 AService 的早期引用。
这个早期引用可能是原始的 AService 对象，也可能是代理对象（如果需要进行 AOP 增强）。
将这个早期引用放入二级缓存（earlySingletonObjects）。
7.完成 BService 的创建和初始化：
使用从二级缓存中获取的 AService 的早期引用，完成 BService 的依赖注入和初始化。
将完整的 BService 放入一级缓存（singletonObjects）。
8.继续初始化 AService：
继续对 AService 进行属性赋值，此时 BService 已经完全初始化并在一级缓存中，可以顺利注入。
完成 AService 的剩余初始化步骤，将完整的 AService 放入一级缓存。

为什么需要三级缓存 二级缓存不是也能做到提前暴露吗?
如果不考虑aop的存在 二级缓存确实能解决循环依赖的问题  但是假如说有AOP的存在  我们是需要代理对象来对我们的目标对象来做增强的 而我们的目标对象和代理对象 内存地址是不同  假如没有三级缓存的存在
我们直接在二级缓存暴露的是目标对象的早期引用 还没有进行初始化 也即并不是我们需要的代理对象 最终导致依赖的属性和创建出来对象不是同一个  
而三级缓存的存在 可以帮我们提前aop 假如需要aop 就可以提前把代理对象暴露出来 放到二级缓存中 供我们使用



三级缓存并不能解决构造器注入的方式的循环依赖  
构造器注入:构造器注入要求在实例化时提供所有依赖

bean的实例化过程
首先，通过 BeanDefinition 可以获取到对应的 className 信息。
然后，利用反射机制获取对应的 Class 对象。
接下来，获取 Class 对象中的构造函数。
最后，通过构造函数来实例化对象。

bean的生命周期
1.通过beanDefinition获得bean的className 通过反射获得class对象 然后再通过class对象获得构造方法 通过构造方法实例化对象
2.依赖注入属性（@Autowired、@Value）
3.假如bean还实现了aware接口（对bean做一些特殊处理） 还需要调用相关aware接口方法
4.初始化之前调用beanPostProcessor中的before方法
5.初始化（@PostConstruct @InitializingBean（afterPropertiesSet）自定义的init方法）
6.初始化后调用BeanPostProcessor中的after方法（AOP）
7.bean创建完成
8.销毁bean



属性注入与初始化
属性注入和初始化是两个不同的概念，虽然它们在 Spring Bean 的生命周期中密切相关。
1. 属性注入
   属性注入是指在一个 Bean 实例化之后，Spring 将依赖的其他 Bean 或属性值注入到该 Bean 中的过程。这是为了确保 Bean 拥有其正常运行所需的所有依赖。Spring 支持多种方式的依赖注入，包括构造器注入、Setter 方法注入和字段注入。
2. 初始化
   初始化是指在属性注入完成之后，Spring 对 Bean 执行的额外配置和设置过程。初始化阶段可能包括以下内容：
   2.1 调用 BeanPostProcessor：
   BeanPostProcessor 是 Spring 提供的一种机制，允许在 Bean 初始化前后进行自定义操作。常见的操作包括 AOP 代理的创建等。
   postProcessBeforeInitialization：在 Bean 的初始化方法调用之前执行。
   postProcessAfterInitialization：在 Bean 的初始化方法调用之后执行。
   2.2 调用自定义初始化方法：
   如果 Bean 类中定义了一个自定义初始化方法（例如，通过 @PostConstruct 注解或实现 InitializingBean 接口的 afterPropertiesSet 方法），Spring 会在属性注入完成后调用该方法。
   2.3 处理 AOP 代理：
   在初始化过程中，Spring 会检查是否需要为 Bean 创建 AOP 代理，并在必要时创建代理对象。


BeanPostProcessor  spring提供的接口 为bean的每个生命周期提供钩子 来对bean做增强或者特殊处理  
BeanFactoryPostProcessor  spring提供的接口  发生在所有bean实例化之前 获取bean的定义信息 BeanDefinition 可以改变bean的元信息 如属性值 作用域 初始化方法等
instantiationAwarePostProcessor	 继承BeanPostProcessor  除了postProcessBeforeInitialization postProcessAfterInitialization 还提供了postProcessBeforeInstantiation postProcessAfterInstantiation postProcessProperties
postProcessBeforeInstantiation:发生在实例化之前
postProcessAfterInstantiation:发生在实例化之后
postProcessProperties:发生在属性注入(populateBean)之前


Bean 的生命周期中各阶段的调用时机
实例化前：
InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation

实例化：
通过构造函数(反射调用无参构造)或工厂方法创建 Bean 实例。

实例化后，属性填充前：
InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation
InstantiationAwareBeanPostProcessor.postProcessProperties

属性填充：
populateBean 方法执行，进行依赖注入。

初始化前：
BeanPostProcessor.postProcessBeforeInitialization

初始化：
调用 Bean 的初始化方法（如 afterPropertiesSet 或 init-method）。

初始化后：
BeanPostProcessor.postProcessAfterInitialization

Bean 就绪：
Bean 已经可以被应用程序使用。

销毁（如果适用）：
应用程序上下文关闭时，调用 Bean 的销毁方法（如 destroy 或 destroy-method）。

spring中的bean是线程安全的吗？
不是，在spring中bean默认都是单例的，假如在bean中定义了可被修改的成员变量，那这个bean就是有状态的bean使用线程安全的 换句话说有状态的bean是非线程安全的 无状态的bean是线程安全的
假如bean必须得定义有状态的bean 又想要线程安全  则可将bean定义成原型类型（prototype  每次注入都是一个新的实例 从而避免了线程安全问题）


AOP：
面向切面编程   将非业务相关的一些公共代码抽取出来 来完成对目标方法的增加  例如：记录日志、事务处理等    降低与业务代码的耦合

spring中事务失效的场景
1.异常被捕获  （可以在catch中重新抛出异常）
2.抛出检查异常（例如IOException 编译器在编译期间强制要求抛出的异常  可以在@Transcational注解中声明会滚异常为Exception）
3，非public方法也可能会造成事务失效 （事务是利用了aop aop的底层实现是动态代理 动态代理有jdk动态代理 依赖接口 接口中的方法都是public abstract 所以非public方法不写 而cglib是使用目标类的字类作为代理对象 所以可以代理public default 但是private不行）


-----------------设计模式---------------------------------

开闭原则: 不修改现有代码的前提下,可以通过拓展(创建新的工具类) 来添加新的功能


简单工厂模式:一个大的工厂 创建所有类型的对象 (缺点:违反了OCP 不易于拓展)
工厂方法模式: 多个工厂(可拓展增加),每个工厂创建一种类型的对象(缺点:每个类型对应一个工厂 工厂数量会越来越多)  单一职责
抽象工厂模式: 多个工厂,每个工厂可以创建同一产品族(多种类型 但是属于同一族系)的对象  
(在一定程度上 能解决工厂方法模式中 工厂越来越多的情况 缺点:产品族增加 每个工厂类也会随之添加新的方法)
抽象工厂模式也会面临OCP问题 但是没有简单工厂模式遇到的那么频繁 因为只有产品族的增加 才会影响到抽象工厂模式OCP 这也是一种折中、权衡的体现


spring中使用了哪些设计模式
1.单例模式
2.工厂模式（FactoryBean）
3.代理模式（AOP）
4.责任链模式（拦截器、过滤器）
5.模板方法模式（redisTemplate）
6.策略模式

----------------------------------------------redis------------------------------------------

缓存穿透: 访问缓存\db都不存在的数据			解决方案:1.不存在数据以null缓存并返回(数据不一致 不存在后来变存在);2.布隆过滤器(redisson 位图 多个hash算法 对位图取模得到位置 同为一则缓存存在 可去访问缓存 有误判 可设置误判容忍度)
缓存击穿: 某一时刻热点数据过期 大量请求过来		解决方案:	1.互斥锁(加锁 阻塞直到缓存重建) 强一致  2.设置逻辑过期时间(其实也是加锁,只是不阻塞,返回过期数据) 高可用
缓存雪崩: 热点数据同一时间过期或redis宕机		解决方案: 1.热点数据过期时间随机    2.redis做高可用   3.nginx或者springcloud gateway做降级限流


双写一致性：  延时双删(删除缓存  操作数据库  延时再次删除缓存（操作数据库期间 有读操作读取旧数据到缓存中  所以需再次删除  延时意义：数据库为主从架构 等待主从同步 也是等待 操作数据库期间 缓存被赋予旧数据操作完成）)    不能保证数据一致性（延时时间不好控制 最终数据可能还是不一致）
1、保证强一致性： 使用redisson提供的读写锁 readWriteLock   读读共享 读写互斥  写写互斥  （保证在操作数据库期间 不可能读到脏数据）   适合读多写少  大多数使用缓存也是读多写少   读少写多 效率就比较低下了
2.保证最终一致性即可（允许短时间内数据不一致）方案就较多了  2.1 使用消息队列  操作数据库时  给缓存发送消息  更新数据
2.2  使用阿里的canal  canal服务会将己伪装成数据库的一个节点 并监听binlog日志  然后将数据库的变更操作告诉redis redis重放数据库操作   好处：代码侵入性较低



redis分布式锁
// 分布式锁的出现是由于现代的系统架构逐渐从单体架构转变为分布式架构。
// 本地锁（如 synchronized、ReentrantLock）只能保证在相同 JVM 下的互斥性，不能跨 JVM 保证互斥性，因此需要分布式锁。
// Redisson 实现的分布式锁，底层是通过 Redis 的 setnx 命令（set if not exists，如果不存在则添加）和 Lua 脚本实现的。
// Lua 脚本用于保证 Redis 中多个指令的原子性执行，确保设置锁和过期时间是一个不可分割的操作。
// 指令格式：set lock myLock nx ex 30：添加互斥锁并设置过期时间。nx 代表互斥，ex 代表锁的过期时间。
// 通过该指令，设置锁的同时设置过期时间（为了避免死锁），这个指令的原子性确保了设置锁和设置过期时间是一个不可分割的操作。
// 然而，过期时间不好控制：设置太短，程序可能在执行完之前就释放了锁；设置太长，会影响其他线程加锁。
// Redisson 提供了更强大的分布式锁机制，利用 Watchdog（看门狗）机制，使得程序员不再需要关注锁的过期时间，而是由 Watchdog 来控制。
// Watchdog 默认将锁的过期时间设置为 30 秒，并且每隔 10 秒检查一次。如果程序还没执行完，则会自动给锁续期 30 秒。
// 程序员只需要在程序执行完毕后手动释放锁即可。
// Watchdog 的底层实现是 Redisson 启动了一个调度任务，每隔 10 秒检查一次锁的持有状态。如果超过 10 秒锁还未被释放，则续期 30 秒。
// Redisson 提供的分布式锁是支持重入的。底层使用一个 hash 数据结构存储，key 为锁的名称，value 为线程 id 和重入次数。重入过程类似于 ReentrantLock 提供的重入锁。
// Redisson 提供的分布式锁不能保证主从一致性。Redisson 提供的红锁（RedLock）可以保证一致性，但实现较为复杂，需要每个 Redis 实例同时持有锁，性能较低。
// Redis 主从架构遵循 AP 协议，目的是高可用优先。如果需要强一致性（CP），建议使用 Zookeeper 提供的分布式锁。
//    redisson可以理解成增强redis功能的一个强大的java框架


redis集群
1.redis主从模式   解决高并发读  但是不能高可用  
同步数据：
1全量同步（从节点发起数据同步请求 携带自己的rrepliId和offset repliId和主节点不一致  代表第一次同步 则主节点bgsave 生成rdb文件 从节点清空数据 重放rdb 同步数据）  
2.增量同步（repliId和主节点一致 则不是第一次 做增量同步 同步repl_backLog(主节点一系列数据增量操作 携带offset)日志文件给从节点  从节点从自己的offset之后 重做操作 恢复数据即可）
2.redis哨兵模式  高并发读+高可用  哨兵负责监控和选举   脑裂：由于网络分区问题 出现了多个master节点 网络问题恢复会导致脑裂期间写数据的丢失  解决：1.设置一个master节点必须有一个slave节点 负责拒绝写入   2.设置主从复制延迟时间不大于5s
3.redis分片集群模式  多个master节点  每个master节点都有自己的主从架构  各个master节点相互监控(相当于哨兵) hash槽做路由(16484个hash槽 分配给多个master节点 数据过来 先做crc16hash算法 然后对16484取模 确定数据落在哪个master节点) 高并发读+高并发写+高可用+海量数据存储
如果主要关注读性能且不需要高可用性，可以选择主从模式。
如果需要高可用性且集群规模不大，可以选择哨兵模式。
如果需要处理海量数据和高并发读写，同时需要高可用性，可以选择分片集群模式。

redis是单线程的为什么还这么快?
1.首先redis是基于内存的,内存的读写速度快,这是最主要的原因
2.单线程避免的线程上下文的切换
3.redis使用了多路复用的IO模型，使得它能够在单线程中高效地处理大量并发连接。
用户态(空间): 应用程序运行的空间，受限于操作权限，不能直接进行硬件操作，需要通过系统调用与内核态进行交互。
内核态(空间): 操作系统内核运行的空间，具有完全的操作权限，可以直接访问硬件和管理系统资源。
我们应用程序想要做某件事 其实最根本的目的是去操作硬件的  而在用户态下是不能去操作硬件的 必须得依赖于操作系统 也就是系统调用才行  所以必然面临着用户态和内核态的通信  这是IO模型最主要的性能瓶颈
（以下是一个更详细的例子来展示用户态和内核态之间的数据拷贝：
客户端连接服务器：
客户端调用 connect()，将服务器地址和端口信息从用户态拷贝到内核态。
内核生成并发送 TCP SYN 包，通过网络传输到服务器。
服务器接受连接：
服务器调用 accept()，内核处理来自客户端的 SYN 包并返回 SYN-ACK 包给客户端。
内核将新的客户端连接信息拷贝到用户态，并返回新的套接字描述符。
在这些过程中，数据拷贝的关键在于：
将控制信息（如套接字描述符、地址和端口信息）从用户态传递到内核态。
处理网络传输的数据包时在内核态中进行，确保高效的网络通信。）
BIO: 阻塞IO模型  用户进程通过recvfrom这个系统调用 去请求内核中的数据 此时用户进程是阻塞住的 一直等到内核中的数据准备就绪 数据准备就绪 则从内核空间拷贝到用户空间 这一过程也是阻塞的
NIO:非阻塞IO模型  用户进程通过recvfrom系统调用请求内核 假如说内核空间中数据未准备就绪 则会直接返回一个异常给用户进程  用户进程接受到这个异常后继续发送recvfrom请求  这个过程持续到内核中数据准备就绪  相较于BIO虽然说是非阻塞的 但是性能也不高  用户进程处于一个忙等待 cpu占有率会比较高
IO多路复用: 通过单线程去监听多个socket  当一个或者多个socket可读或者可写就去通知用户进程 充分利用了cpu资源 效率比较高
IO多路复用的三种模式:select、poll、epoll
select：传统的IO多路复用模型  虽然监听了多个socket  但是并不知道具体哪个socket准备就绪  需要轮询遍历  而且有文件描述符的限制 通常是1024
poll：与select相比虽然没有文件描述符的限制 但是仍然不知道具体哪个socket准备就绪 仍需要轮询遍历
epoll：才有事件通知的机制 准确知道哪个socket准备就绪 并将准备就绪的socket拷贝到用户空间  且没有文件描述符的限制
目前redis会根据不同的操作系统 选择最优的IO多路复用模型  可在redis命令行通过info命令查看用到了哪个模型
目前redis6.0虽然引入了多线程  但是核心事件循环和命令执行还是单线程的  多线程是使用在命令的接收和命令的回复上
延伸面试题：redis单线程是怎么处理大量并发连接的（多路复用）


------------------------------------------分布式架构------------------------------------------------------------
1.什么是接口幂等？如何实现接口幂等
接口幂等：一个接口不管调用多少次 最终的结果不会发生改变 都和第一次调用一样
如何解决：1.数据库唯一索引（天然幂等 但是只能解决新增）  2，token+redis（每次请求携带一个唯一的Token，在处理请求前先检查Redis中是否存在该Token。如果存在，表示该操作已经执行过，直接返回结果；如果不存在，则执行操作，并将Token存入Redis，操作完成后删除Token）   3.分布式锁（每次获取锁都会查看操作状态（可通过设置标识位） 操作成功了 则后续获取锁请求则不再处理）


2.BASE理论
base理论用来指导CAP理论中一致性和可用性之间权衡的一种方案  base理论需要分布式架构保证：1.系统基本可用 2.软状态（允许短期内 数据不一致）  3.最终一致（数据在最终状态下达到数据一致即可）

3.微服务和分布式的区别
一句话：微服务是一种架构风格 而分布式是一种部署方式
微服务：一种架构风格，强调将系统按照业务功能拆分为多个独立的服务，每个服务可以独立开发、部署和扩展。ps：每个服务可以是分布式的部署在多台机器上
分布式：一种部署方式，系统的组件分布在网络中的多台计算机上，通过消息传递进行通信和协调，提升系统的并发处理能力和容错性。
ps：微服务通常是分布式的  而分布式架构不一定是微服务

4.seata的各种模式
XA模式：TM开启全局事务，RM注册到TC上，第一阶段TM会通知各RM准备提交，RM开始处理各自的业务，但是不提交只是告诉TM自己准备好了，第二阶段等所有的分支事务全部准备就绪则TM向TC提交全局事务，然后TC开始检查各分支事务的状态
如果没问题就通知各分支事务可以提交了，如果有问题则通知各分支事务回滚  CP思想
AT模式： TM开启全局事务，RM注册到TC上，各分支事务开始处理自己的业务并记录到undolog日志中然后提交事务，TC检查各分支事务的状态，如果没问题则通知各分支事务删除相应的undolog记录，如果有问题则通知各分支事务通过undolog
日志进行回滚事务  AP思想  seata的默认模式 @GlobalTransactional
TCC模式：TM开启全局事务 RM注册到TC上 各分支事务try进行资源预留 然后报告自己的事务状态给TC TC检查各分支事务状态 都try成功则进行confirm操作即操作业务提交事务，如果try失败则进行cancel操作释放资源即回滚操作
try、confirm、cancel都需要写代码 代码倾入性较高  AP思想

XA模式
TM开启全局事务：TM生成全局事务ID并通知各RM。
RM注册到TC上：各RM注册自己的分支事务到TC。
第一阶段（准备阶段）：
TM通知各RM准备提交。
各RM执行本地事务但不提交，并告知TM自己准备好了。
第二阶段（提交/回滚阶段）：
如果所有分支事务都准备就绪，TM通知TC提交全局事务。
TC检查各分支事务的状态，如果没问题，则通知各分支事务提交；如果有问题，则通知各分支事务回滚。
CP思想：注重一致性（Consistency）和分区容忍性（Partition Tolerance），即使牺牲可用性（Availability）。
AT模式
TM开启全局事务：TM生成全局事务ID并通知各RM。
RM注册到TC上：各RM注册自己的分支事务到TC。
各分支事务处理业务：
各分支事务处理自己的业务逻辑，并记录操作到Undo Log中，然后提交事务。
TC检查分支事务状态：
在提交阶段，TC检查各分支事务的状态。
如果没有问题，则通知各分支事务删除相应的Undo Log记录。
如果有问题，则通知各分支事务通过Undo Log进行回滚。
AP思想：注重可用性（Availability）和分区容忍性（Partition Tolerance），通过Undo Log机制实现数据一致性。Seata的默认模式。
TCC模式
TM开启全局事务：TM生成全局事务ID并通知各RM。
RM注册到TC上：各RM注册自己的分支事务到TC。
Try阶段：
各分支事务执行资源预留操作，并报告状态给TC。
Confirm/Cancel阶段：
如果所有分支事务的Try操作都成功，TC通知各分支事务执行Confirm操作，提交事务。
如果有任何一个分支事务的Try操作失败，TC通知各分支事务执行Cancel操作，回滚事务，释放资源。
AP思想：需要业务方编写Try、Confirm、Cancel代码，灵活性高，代码侵入性较高。
补充说明
XA模式和AT模式主要是由TM负责全局事务的启动和提交，TC负责状态管理和协调。
TCC模式涉及业务逻辑的三阶段操作（Try、Confirm、Cancel），需要业务方额外实现这些操作。




----------------------------------------数据结构-------------------------------------------------------------------
红黑树（也叫二叉平衡树）
红黑树的特点：
1.节点颜色要么黑色要么红色
2.根节点黑色
3.红色节点的子节点为黑色
4.所有的叶子节点都为null的黑色节点
5.从任意节点出发到他的叶子节点的所有路径中  经过的黑色节点数量都是相同的
当红黑色添加或者删除元素时  都需要旋转来达到这以上几点特点 来达到相对稳定的平衡二叉树（相较于二叉搜索树（左边节点小于右边节点）而言（二叉搜索树在极端情况下会退化为链表 插入删除时间复杂度就成为O（n）））  时间复杂度为O（log n）


HashMap
在jdk1.8之前  HashMap结构为数据+链表    1.8之后是数组+链表+红黑树
hashMap put元素的流程
1.第一次put 则会执行resize方法完成hashMap的初始化（初始数组长度为16）
2.计算元素key的hash值来确定 在数组上的下标 假如数组下标位置没有元素 则直接在数组下标出添加元素即可
3.假如数组下标处有元素，则判断数组下标处的数据结构，
3.1假如是红黑树，则遍历红黑色，查找是否有相同的key，假如有则直接覆盖value，假如没有则在红黑树上插入元素
3.2假如数组下标处的数据结构是链表则遍历该链表查找是否有key相同的元素如果有则直接覆盖，如果没有则在链表的尾部插入元素，然后判断链表的长度是否大于8且数组的长度是否大于64如果是 则链表需要转为红黑树
4.插入成功后需判断加载因子0.75*数组的长度是否超过hashMap中的键值对总数 如果超过了则需要对hashMap进行扩容
ps：为啥链表是否树化 还需要再判断数组的长度是否大于64？ 因为假如数组长度小于64 我们可以通过扩容来增大数组的长度 从而使得数组该位置处的链表长度变短来提高效率 而不需要进行树化 因为树化还是比较复杂且耗时的

在 HashMap 扩容时，无论是链表还是红黑树都可能面临拆分的问题。
HashMap 的底层是由数组、链表和红黑树组成的。当进行扩容操作时，不仅会扩大数组的容量，还会对所有元素重新计算哈希值，因为长度扩大后，哈希值也会随之改变。
对于链表的处理，会将原有链表拆分成两个链表。通过位运算来决定元素在扩容后的位置：如果元素的哈希值与旧数组容量进行位与操作的结果为 0，则该元素存放在低位（原索引位置）；如果结果为 1，则存放在高位（原索引 + 旧数组长度）。
对于红黑树的处理，其拆分逻辑与链表基本一致。在重新映射后，也会将红黑树拆分成两条链表，然后根据链表的长度，判断是否需要把链表重新进行树化。如果链表长度小于等于红黑树转链表的阈值（默认为 6），则将红黑树转成链表；否则保留红黑树结构。

这样的处理方式可以在一定程度上提高 HashMap 在扩容时的性能，减少元素的重新排列和调整次数。

之所以要使用链表和红黑树来实现 HashMap，主要是为了解决哈希值重复的问题。在哈希冲突较少时，链表的结构性能较高；而当链表长度较长（不小于 8）且数组长度不小于 64 时，将链表转换为红黑树可以提高检索速度。然而，HashMap 频繁的扩容会导致底部红黑树不断地进行拆分和重组，这是比较耗时的操作。因此，只有在链表长度较长时转换为红黑树才会显著提高效率。


----------------------------并发编程-------------------------------------------------------------
synchronized锁升级的过程
第一次没有线程访问他 是无锁的状态 当有线程来访问则升级成偏向锁 对象锁的markword中会记录该线程的id 此后该线程再次访问则只需要判断和对象锁中的markword记录的线程id是否一致即可，假如不一致 则证明有新的线程来竞争了则升级成轻量级锁，当升级成轻量级锁之后，线程是通过cas自旋方式来获取对象锁 假如自旋次数较多仍无法获取到锁 代表竞争比较激烈 则会升级成重量级锁，
重量级锁是通过Monitor对象监视器来实现的，直接会让获取不到锁的线程进入Monitor对象中的entryList来进行排队等待来保证同一时间只有一个线程获取到锁


reentrantLock加锁逻辑：
ReentrantLock 通常通过一次 CAS 操作（判断state字段是否为0）尝试获取锁，如果失败且锁被其他线程持有，线程会立即进入 AQS 的等待队列。这种设计避免了大量无效的 CAS 重试，提升了锁竞争激烈时的性能。
非公平锁：一次 CAS 操作失败后，如果锁被其他线程持有，线程会立即进入等待队列。
公平锁：检查等待队列，如果有其他线程等待，则当前线程进入等待队列，确保公平性。

synchronized和reentrantLock的区别
1.synchronized是jvm提供的锁   ReentrantLock是java实现的锁
2.ReentrantLock可支持被中断（lock.lockInterruptibly()）  synchronized不可以
3.ReentrantLock可以设置获取锁的超时等待时间 而synchronized没有相应的操作
4.ReentrantLock可以实现公平锁  而synchronized只能是非公平锁
5.ReentrantLock可以设置多个条件变量配合使用  而synchronized没有相应操作
6.ReentrantLock需要手动去释放锁  而synchronized不需要

强软弱虚引用
强引用：不会被回收  宁可OOM
软引用：内存不够 gc才会回收
弱引用：只要gc就会被回收 例如ThreadLocal中的静态内部类ThreadLocalMap 而ThreadLocalMap中的内部类Entry他的key是个弱引用
虚引用：用于跟踪对象的垃圾回收 通常配合引用队列使用

ThreadLocal
作用：用于线程间隔离的一块内存空间  避免并发安全问题   例如Spring中管理jdbc连接 用的就是ThreadLocal 每个线程用自己的ThreadLocal管理自己的connection连接 从而不会关闭/影响其他线程的连接
原理: 每个线程都有一个ThreadLocalMap属性 ThreadLocalMap中存储的是自定义的Entry对象 key为ThreadLocal value为自己想要存储的值 每次set或者get都是操作自己线程的ThreadLocalMap 从而做到线程隔离
ThreadLocal中的内存泄露问题:
static class Entry extends WeakReference<ThreadLocal<?>> {
/** The value associated with this ThreadLocal. */
Object value;

            Entry(ThreadLocal<?> k, Object v) {
                super(k);	//弱引用
                value = v;	//强引用
            }
ThreadLocalMap中的Entry 他的key是个弱引用 而value是个强引用 发生gc时key会被回收  而value不会
解决:  每次使用完 记得remove手动释放一下       
		


